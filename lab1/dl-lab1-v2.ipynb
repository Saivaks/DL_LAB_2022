{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch.utils.data import DataLoader\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nfrom torchvision.io import read_image\nfrom torchvision import datasets, models, transforms\nimport torch\nimport torch.nn as nn\nimport scipy.io\nimport os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\nimport json\nimport cv2\nfrom IPython.display import FileLink\nfrom PIL import Image\nimport torchvision\nfrom torch.optim.optimizer import Optimizer\nfrom typing import List, Optional\nfrom torch import Tensor\nfrom torchmetrics import F1Score\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by cimport scipy.iolicking run or pressing Shift+Enter) will list all files under the input directory\n\n%ls /kaggle/input/cars-v2/LR1-1/\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-15T11:24:55.503900Z","iopub.execute_input":"2022-12-15T11:24:55.504353Z","iopub.status.idle":"2022-12-15T11:25:04.040516Z","shell.execute_reply.started":"2022-12-15T11:24:55.504266Z","shell.execute_reply":"2022-12-15T11:25:04.039278Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"README.txt     cars_test_annos.mat   eval_train.m\ncars_meta.mat  \u001b[0m\u001b[01;34mcars_train\u001b[0m/           train_perfect_preds.txt\n\u001b[01;34mcars_test\u001b[0m/     cars_train_annos.mat\n","output_type":"stream"}]},{"cell_type":"code","source":"class CarsDataset(torch.utils.data.Dataset):\n    def __init__(self, annotations, classes = None, img_folder_path = '',\n                 transform=None, target_transform=None):\n        \n        self.annotations = annotations\n        self.transform = transform\n        self.target_transform = target_transform\n        self.img_folder_path = img_folder_path\n        self.classes = classes\n\n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, idx):\n        x1, y1, x2, y2, class_number, img_name = self.annotations[idx]\n        uncropped_image = torchvision.io.read_image(\n            os.path.join(self.img_folder_path, img_name))\n        image = uncropped_image[:, y1:y2, x1:x2]\n        #image = image.to(dtype = torch.float32)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            class_number = self.target_transform(class_number)\n        return image, class_number - 1 # В аннтоациях числа от 1 до 196\n    \nclass CarsDatasetWithoutLabels(CarsDataset):\n    def __getitem__(self, idx):\n        x1, y1, x2, y2, img_name = self.annotations[idx]\n        uncropped_image = torchvision.io.read_image(\n            os.path.join(self.img_folder_path, img_name))\n        image = uncropped_image[:, y1:y2, x1:x2]\n        # image = image.to(dtype = torch.float32)\n        if self.transform:\n            image = self.transform(image)\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:25:59.415113Z","iopub.execute_input":"2022-12-15T11:25:59.416055Z","iopub.status.idle":"2022-12-15T11:25:59.428125Z","shell.execute_reply.started":"2022-12-15T11:25:59.416015Z","shell.execute_reply":"2022-12-15T11:25:59.427188Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset_loc = '/kaggle/input/cars-v2/LR1-1/'\n\noriginal_train_annos = scipy.io.loadmat(\n    os.path.join(dataset_loc, 'cars_train_annos.mat'),\n    squeeze_me = True\n)['annotations']\n\noriginal_test_annos = scipy.io.loadmat(\n    os.path.join(dataset_loc, 'cars_test_annos.mat'),\n    squeeze_me = True\n)['annotations']\n\nclass_names = scipy.io.loadmat(\n    os.path.join(dataset_loc, 'cars_meta.mat'),\n    squeeze_me = True\n)['class_names']","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:02.743056Z","iopub.execute_input":"2022-12-15T11:26:02.743587Z","iopub.status.idle":"2022-12-15T11:26:03.457244Z","shell.execute_reply.started":"2022-12-15T11:26:02.743542Z","shell.execute_reply":"2022-12-15T11:26:03.456034Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:04.721501Z","iopub.execute_input":"2022-12-15T11:26:04.722133Z","iopub.status.idle":"2022-12-15T11:26:04.733113Z","shell.execute_reply.started":"2022-12-15T11:26:04.722042Z","shell.execute_reply":"2022-12-15T11:26:04.732024Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_transforms = dict()\ndata_transforms['train'] = (\n    torchvision.transforms.Compose([\n        torchvision.transforms.ToPILImage(),\n        #минимальный размер изображения для Inception\n        torchvision.transforms.Resize((299, 299)),\n        torchvision.transforms.Grayscale(num_output_channels=3),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n    ]))\n\ndata_transforms['test'] = (\n    torchvision.transforms.Compose([\n        torchvision.transforms.ToPILImage(),\n        torchvision.transforms.Resize((299, 299)),\n        torchvision.transforms.Grayscale(num_output_channels=3),\n        torchvision.transforms.ToTensor(),\n    ]))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:06.190783Z","iopub.execute_input":"2022-12-15T11:26:06.191170Z","iopub.status.idle":"2022-12-15T11:26:06.198695Z","shell.execute_reply.started":"2022-12-15T11:26:06.191122Z","shell.execute_reply":"2022-12-15T11:26:06.197696Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data_transforms['train']","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:08.204539Z","iopub.execute_input":"2022-12-15T11:26:08.204943Z","iopub.status.idle":"2022-12-15T11:26:08.214058Z","shell.execute_reply.started":"2022-12-15T11:26:08.204908Z","shell.execute_reply":"2022-12-15T11:26:08.212915Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Compose(\n    ToPILImage()\n    Resize(size=(299, 299), interpolation=bilinear, max_size=None, antialias=None)\n    Grayscale(num_output_channels=3)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)"},"metadata":{}}]},{"cell_type":"code","source":"datasets = {\n    phase: CarsDataset(original_train_annos, img_folder_path = os.path.join(dataset_loc, 'cars_train'), \n                       transform = data_transforms[phase], classes = class_names)\n    for phase in ['train']\n}\n#datasets['train'] = CarsDataset(original_train_annos, img_folder_path = os.path.join(dataset_loc, 'cars_train'),transform = data_transforms['train'], \n#                                classes = class_names)\ndatasets['test'] = CarsDatasetWithoutLabels(\n    original_test_annos,\n    img_folder_path = os.path.join(dataset_loc, 'cars_test'),\n    classes = class_names)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:09.635934Z","iopub.execute_input":"2022-12-15T11:26:09.636324Z","iopub.status.idle":"2022-12-15T11:26:09.642893Z","shell.execute_reply.started":"2022-12-15T11:26:09.636292Z","shell.execute_reply":"2022-12-15T11:26:09.641897Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"n_classes = len(datasets['train'].classes)\nn_classes","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:37:28.897966Z","iopub.execute_input":"2022-12-15T11:37:28.898527Z","iopub.status.idle":"2022-12-15T11:37:28.910817Z","shell.execute_reply.started":"2022-12-15T11:37:28.898485Z","shell.execute_reply":"2022-12-15T11:37:28.909677Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"196"},"metadata":{}}]},{"cell_type":"code","source":"dataloaders = {\n    'train':\n    torch.utils.data.DataLoader(datasets['train'],\n                                batch_size=16,\n                                shuffle=True,\n                                num_workers=0),  # for Kaggle\n    'test':\n    torch.utils.data.DataLoader(datasets['test'],\n                                batch_size=16,\n                                shuffle=False,\n                                num_workers=0)  # for Kaggle\n}","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:11.481647Z","iopub.execute_input":"2022-12-15T11:26:11.482047Z","iopub.status.idle":"2022-12-15T11:26:11.487806Z","shell.execute_reply.started":"2022-12-15T11:26:11.482015Z","shell.execute_reply":"2022-12-15T11:26:11.486716Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(datasets['train']), len(datasets['test'])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:15.669795Z","iopub.execute_input":"2022-12-15T11:26:15.670178Z","iopub.status.idle":"2022-12-15T11:26:15.676664Z","shell.execute_reply.started":"2022-12-15T11:26:15.670123Z","shell.execute_reply":"2022-12-15T11:26:15.675745Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(8144, 8041)"},"metadata":{}}]},{"cell_type":"code","source":"# один из вариантов реализации Inception v3\nclass GridReduction(nn.Module):\n    def __init__(self, in_fts, out_fts):\n        super(GridReduction, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts, kernel_size=(3, 3), stride=(2, 2))\n        )\n\n        self.branch2 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n        )\n\n    def forward(self, input_img):\n        o1 = self.branch1(input_img)\n        o2 = self.branch2(input_img)\n        x = torch.cat([o1, o2], dim=1)\n        return x\n\n#блок со сверткой 3x3 остальные модули по аналогии\nclass Inceptionx3(nn.Module):\n    def __init__(self, in_fts, out_fts):\n        super(Inceptionx3, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[0], kernel_size=(1, 1), stride=(1, 1)),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(3, 3), stride=(1, 1), padding=1),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(3, 3), stride=(1, 1), padding=1)\n        )\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[1], kernel_size=(1, 1), stride=(1, 1)),\n            nn.Conv2d(in_channels=out_fts[1], out_channels=out_fts[1], kernel_size=(3, 3), stride=(1, 1), padding=1),\n        )\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(kernel_size=(3, 3), stride=(1, 1), padding=1),\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[2], kernel_size=(1, 1), stride=(1, 1))\n        )\n        self.branch4 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[3], kernel_size=(1, 1), stride=(1, 1))\n        )\n\n    def forward(self, input_img):\n        o1 = self.branch1(input_img)\n        o2 = self.branch2(input_img)\n        o3 = self.branch3(input_img)\n        o4 = self.branch4(input_img)\n        x = torch.cat([o1, o2, o3, o4], dim=1)\n        return x\n\n\nclass Inceptionx5(nn.Module):\n    def __init__(self, in_fts, out_fts, n=7):\n        super(Inceptionx5, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[0], kernel_size=(1, 1), stride=(1, 1)),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(1, n), stride=(1, 1),\n                      padding=(0, n // 2)),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(n, 1), stride=(1, 1),\n                      padding=(n // 2, 0)),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(1, n), stride=(1, 1),\n                      padding=(0, n // 2)),\n            nn.Conv2d(in_channels=out_fts[0], out_channels=out_fts[0], kernel_size=(n, 1), stride=(1, 1),\n                      padding=(n // 2, 0)),\n        )\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[1], kernel_size=(1, 1), stride=(1, 1)),\n            nn.Conv2d(in_channels=out_fts[1], out_channels=out_fts[1], kernel_size=(1, n), stride=(1, 1),\n                      padding=(0, n // 2)),\n            nn.Conv2d(in_channels=out_fts[1], out_channels=out_fts[1], kernel_size=(n, 1), stride=(1, 1),\n                      padding=(n // 2, 0)),\n        )\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(kernel_size=(3, 3), stride=(1, 1), padding=1),\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[2], kernel_size=(1, 1), stride=(1, 1))\n        )\n        self.branch4 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[3], kernel_size=(1, 1), stride=(1, 1))\n        )\n\n    def forward(self, input_img):\n        o1 = self.branch1(input_img)\n        o2 = self.branch2(input_img)\n        o3 = self.branch3(input_img)\n        o4 = self.branch4(input_img)\n        x = torch.cat([o1, o2, o3, o4], dim=1)\n        return x\n\n\nclass Inceptionx2(nn.Module):\n    def __init__(self, in_fts, out_fts):\n        super(Inceptionx2, self).__init__()\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[0] // 4, kernel_size=(1, 1)),\n            nn.Conv2d(in_channels=out_fts[0] // 4, out_channels=out_fts[0] // 4, kernel_size=(3, 3), stride=(1, 1),\n                      padding=1)\n        )\n        self.subbranch1_1 = nn.Sequential(\n            nn.Conv2d(in_channels=out_fts[0] // 4, out_channels=out_fts[0], kernel_size=(1, 3), stride=(1, 1),\n                      padding=(0, 3 // 2))\n        )\n        self.subbranch1_2 = nn.Sequential(\n            nn.Conv2d(in_channels=out_fts[0] // 4, out_channels=out_fts[1], kernel_size=(3, 1), stride=(1, 1),\n                      padding=(3 // 2, 0))\n        )\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[2] // 4, kernel_size=(1, 1))\n        )\n        self.subbranch2_1 = nn.Sequential(\n            nn.Conv2d(in_channels=out_fts[2] // 4, out_channels=out_fts[2], kernel_size=(1, 3), stride=(1, 1),\n                      padding=(0, 3 // 2))\n        )\n        self.subbranch2_2 = nn.Sequential(\n            nn.Conv2d(in_channels=out_fts[2] // 4, out_channels=out_fts[3], kernel_size=(3, 1), stride=(1, 1),\n                      padding=(3 // 2, 0))\n        )\n        self.branch3 = nn.Sequential(\n            nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=1),\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[4], kernel_size=(1, 1), stride=(1, 1))\n        )\n        self.branch4 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=out_fts[5], kernel_size=(1, 1), stride=(1, 1))\n        )\n\n    def forward(self, input_img):\n        o1 = self.branch1(input_img)\n        o11 = self.subbranch1_1(o1)\n        o12 = self.subbranch1_2(o1)\n        o2 = self.branch2(input_img)\n        o21 = self.subbranch2_1(o2)\n        o22 = self.subbranch2_2(o2)\n        o3 = self.branch3(input_img)\n        o4 = self.branch4(input_img)\n        x = torch.cat([o11, o12, o21, o22, o3, o4], dim=1)\n        return x\n\n\nclass AuxClassifier(nn.Module):\n    def __init__(self, in_fts, num_classes):\n        super(AuxClassifier, self).__init__()\n        self.pool = nn.AdaptiveAvgPool2d(output_size=(5, 5))\n        self.conv = nn.Conv2d(in_channels=in_fts, out_channels=128, kernel_size=(1, 1))\n        self.classifier = nn.Sequential(\n            nn.Linear(5 * 5 * 128, 1024),\n            nn.BatchNorm1d(num_features=1024),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        N = x.shape[0]\n        x = self.pool(x)\n        x = self.conv(x)\n        x = x.reshape(N, -1)\n        x = self.classifier(x)\n        return x\n\n\nclass MyInception_v3(nn.Module):\n    def __init__(self, in_fts=3, num_classes=1000):\n        super(MyInception_v3, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_fts, out_channels=32, kernel_size=(3, 3), stride=(2, 2)),\n            nn.BatchNorm2d(num_features=32)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.BatchNorm2d(num_features=32)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n            nn.BatchNorm2d(num_features=64)\n        )\n        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=80, kernel_size=(3, 3), stride=(1, 1))\n        self.conv5 = nn.Conv2d(in_channels=80, out_channels=192, kernel_size=(3, 3), stride=(2, 2))\n        self.conv6 = nn.Conv2d(in_channels=192, out_channels=288, kernel_size=(3, 3), stride=(1, 1), padding=1)\n\n        list_incept = [Inceptionx3(in_fts=288, out_fts=[96, 96, 96, 96]),\n                       Inceptionx3(in_fts=4 * 96, out_fts=[96, 96, 96, 96]),\n                       Inceptionx3(in_fts=4 * 96, out_fts=[96, 96, 96, 96])]\n\n        self.inceptx3 = nn.Sequential(*list_incept)\n        self.grid_redn_1 = GridReduction(in_fts=4 * 96, out_fts=384)\n        self.aux_classifier = AuxClassifier(768, num_classes)\n\n        list_incept = [Inceptionx5(in_fts=768, out_fts=[160, 160, 160, 160]),\n                       Inceptionx5(in_fts=4 * 160, out_fts=[160, 160, 160, 160]),\n                       Inceptionx5(in_fts=4 * 160, out_fts=[160, 160, 160, 160]),\n                       Inceptionx5(in_fts=4 * 160, out_fts=[160, 160, 160, 160]),\n                       Inceptionx5(in_fts=4 * 160, out_fts=[160, 160, 160, 160])]\n\n        self.inceptx5 = nn.Sequential(*list_incept)\n        self.grid_redn_2 = GridReduction(in_fts=4 * 160, out_fts=640)\n\n        list_incept = [Inceptionx2(in_fts=1280, out_fts=[256, 256, 192, 192, 64, 64]),\n                       Inceptionx2(in_fts=1024, out_fts=[384, 384, 384, 384, 256, 256])]\n\n        self.inceptx2 = nn.Sequential(*list_incept)\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n        self.fc = nn.Linear(2048, num_classes)\n\n    def forward(self, input_img):\n        N = input_img.shape[0]\n        x = self.conv1(input_img)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.pool(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.inceptx3(x)\n        x = self.grid_redn_1(x)\n        aux_out = self.aux_classifier(x)\n        x = self.inceptx5(x)\n        x = self.grid_redn_2(x)\n        x = self.inceptx2(x)\n        x = self.avgpool(x)\n        x = x.reshape(N, -1)\n        x = self.fc(x)\n        if self.training:\n            return [x, aux_out]\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:26:18.590361Z","iopub.execute_input":"2022-12-15T11:26:18.590735Z","iopub.status.idle":"2022-12-15T11:26:18.640267Z","shell.execute_reply.started":"2022-12-15T11:26:18.590703Z","shell.execute_reply":"2022-12-15T11:26:18.639371Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class AdaSmooth_v1:\n    def __init__(self,\n                 model,\n                 get_params_flat,\n                 set_params_flat,\n                 stepsize=1e-4,\n                 beta1=0.9,\n                 beta2=0.999,\n                 epsilon=1e-12,\n                 alpha=0.95,\n                 momentum=0.9):\n        \n        self.model = model\n        self.t = 0\n        self.stepsize = stepsize\n        self.momentum = momentum\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.alpha = alpha\n\n        self.get_params_flat = get_params_flat\n        self.set_params_flat = set_params_flat\n\n        self.dim = len(self.get_params_flat(model))\n\n        self.m = np.zeros(self.dim, dtype=np.float32)\n        self.v1 = np.zeros(self.dim, dtype=np.float32)\n        self.v2 = np.zeros(self.dim, dtype=np.float32)\n\n    def update(self, grad):\n        self.t += 1\n        step = self._compute_step(grad)\n        theta = self.get_params_flat(self.model)\n        self.set_params_flat(self.model, theta + step)\n\n    def _compute_step(self, grad):\n        # calculate first moment of gradient (momemtum)\n        #self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n        \n        temp_grad = grad.copy()\n        temp_m = list(self.m).copy()\n        print(temp_grad[0])\n        print(temp_m[0])\n        \n        self.m = map(sum, zip(self.beta1 * self.m, (1 - self.beta1) * grad))\n           \n        # calculate second moment of gradient (RMSprop)\n        # use 'np.square(grad - m_t)' for Adabelief instead of 'np.square(grad)'\n        #self.v1 = self.beta2 * self.v1 + (1 - self.beta2) * np.square(grad - self.m)\n        self.v1 = np.concatenate(self.beta2 * self.v1, (1 - self.beta2) * np.square(grad - self.m))\n        # correct bias (mostly affects initial steps)\n        m_corr_t = self.m / (1.0 - np.powerer(self.beta1, self.t))\n        v_corr_t = self.v1 / (1.0 - np.pow(self.beta2, self.t))\n\n        # calculate adaptive step\n        adaptive_step = m_corr_t / (np.sqrt(v_corr_t) + self.epsilon)\n\n        # calculate SGD step\n        #self.v2 = self.momentum * self.v2 + (1. - self.momentum) * grad\n        self.v2 = np.concatenate(self.momentum, self.v2 + (1. - self.momentum) * grad)\n        sgd_step = -self.stepsize * self.v2\n\n        # calculated weighted average step\n        split_factor = np.power(self.alpha, self.t)\n        step = split_factor * sgd_step + adaptive_step * (1 - split_factor)\n\n        # apply lr\n        return self.stepsize * step","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-15T11:26:23.271231Z","iopub.execute_input":"2022-12-15T11:26:23.271929Z","iopub.status.idle":"2022-12-15T11:26:23.284718Z","shell.execute_reply.started":"2022-12-15T11:26:23.271892Z","shell.execute_reply":"2022-12-15T11:26:23.283650Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class AdaSmooth(Optimizer):\n    def __init__(self, params, lr = 0.001, p1 = 0.5, p2 = 0.99, eps=1e-6, weight_decay=0, M = None ):\n        if not lr >= 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not p2 >= p1:\n            raise ValueError(\"p2 must be > p1: p2 = {}, p1 = {}\".format(p2, p1))\n        if not  eps >= 0.0:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not weight_decay >= 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\n        defaults = dict(lr=lr, \n                        p1=p1,\n                        p2=p2, \n                        eps=eps, \n                        weight_decay=weight_decay,\n                        M = M)\n        super(AdaSmooth, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            norm_terms = []\n            xt = []\n            st = []\n            nt = []\n            lr, p1, p2, eps, weight_decay, M = (group['lr'],\n                                            group['p1'],\n                                            group['p2'],\n                                            group['eps'],\n                                            group['weight_decay'],\n                                            group['M'])\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError('Adasmooth does not support sparse gradients')\n                grads.append(p.grad)\n\n                state = self.state[p]\n\n                # Lazy state initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['norm_terms'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state['xt'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state['st'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state['nt'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                state['step'] += 1\n                if state['step'] == M+1:\n                    state['step'] = 0\n                    state['st'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state['nt'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                \n                norm_terms.append(state['norm_terms'])\n                xt.append(state['xt'])\n                st.append(state['st'])\n                nt.append(state['nt'])\n\n            adasmooth(params_with_grad,\n                     grads,\n                     norm_terms,\n                     xt,\n                     st,\n                     nt,\n                     lr=lr,\n                     p1=p1,\n                     p2=p2,\n                     eps=eps,\n                     weight_decay=weight_decay)\n\n        return loss\n\n\ndef adasmooth(params: List[Tensor],\n             grads: List[Tensor],\n             norm_terms: List[Tensor],\n             xt: List[Tensor],\n             st: List[Tensor],\n             nt: List[Tensor],\n             lr: float,\n             p1: float,\n             p2: float,\n             eps: float,\n             weight_decay: float):\n    for (param, grad, norm_term, x, s, n) in zip(params, grads, norm_terms, xt, st, nt):\n        if weight_decay != 0:\n            grad = grad.add(param, alpha=weight_decay)\n\n        if torch.is_complex(param):\n            norm_term = torch.view_as_real(norm_term)\n            grad = torch.view_as_real(grad)\n        s =  torch.add(param - x, s)\n        n = torch.add(torch.abs(param - x), n)\n        er = torch.div(torch.abs(s), n)\n        c = torch.add(torch.mul((p2 - p1), er), (1 - p2)) #8\n        norm_term = torch.add(torch.mul(c ** 2, torch.mul(grad, grad)), torch.mul((1 - c ** 2), norm_term)) #9\n        delta = torch.mul( 1/ torch.sqrt(torch.add(norm_term,eps)),grad) # 10\n\n        if torch.is_complex(param):\n            delta = torch.view_as_complex(delta)\n        param.add_(delta, alpha=-lr) # update","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:33:53.689166Z","iopub.execute_input":"2022-12-15T11:33:53.689591Z","iopub.status.idle":"2022-12-15T11:33:53.711976Z","shell.execute_reply.started":"2022-12-15T11:33:53.689557Z","shell.execute_reply":"2022-12-15T11:33:53.710857Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# версия из торча \nimport warnings\nfrom collections import namedtuple\nfrom functools import partial\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\n\n\n\n\n__all__ = [\"Inception3\", \"InceptionOutputs\", \"_InceptionOutputs\", \"Inception_V3_Weights\", \"inception_v3\"]\n\n\nInceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\nInceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n\n# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n# _InceptionOutputs set here for backwards compat\n_InceptionOutputs = InceptionOutputs\n\n\nclass Inception3(nn.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        aux_logits: bool = True,\n        transform_input: bool = False,\n        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n        init_weights: Optional[bool] = None,\n        dropout: float = 0.5,\n    ) -> None:\n        super().__init__()\n        \n        if inception_blocks is None:\n            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n        if init_weights is None:\n            warnings.warn(\n                \"The default weight initialization of inception_v3 will be changed in future releases of \"\n                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n                \" due to scipy/scipy#11299), please set init_weights=True.\",\n                FutureWarning,\n            )\n            init_weights = True\n        if len(inception_blocks) != 7:\n            raise ValueError(f\"lenght of inception_blocks should be 7 instead of {len(inception_blocks)}\")\n        conv_block = inception_blocks[0]\n        inception_a = inception_blocks[1]\n        inception_b = inception_blocks[2]\n        inception_c = inception_blocks[3]\n        inception_d = inception_blocks[4]\n        inception_e = inception_blocks[5]\n        inception_aux = inception_blocks[6]\n\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.Mixed_5b = inception_a(192, pool_features=32)\n        self.Mixed_5c = inception_a(256, pool_features=64)\n        self.Mixed_5d = inception_a(288, pool_features=64)\n        self.Mixed_6a = inception_b(288)\n        self.Mixed_6b = inception_c(768, channels_7x7=128)\n        self.Mixed_6c = inception_c(768, channels_7x7=160)\n        self.Mixed_6d = inception_c(768, channels_7x7=160)\n        self.Mixed_6e = inception_c(768, channels_7x7=192)\n        self.AuxLogits: Optional[nn.Module] = None\n        if aux_logits:\n            self.AuxLogits = inception_aux(768, num_classes)\n        self.Mixed_7a = inception_d(768)\n        self.Mixed_7b = inception_e(1280)\n        self.Mixed_7c = inception_e(2048)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=dropout)\n        self.fc = nn.Linear(2048, num_classes)\n        if init_weights:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def _transform_input(self, x: Tensor) -> Tensor:\n        if self.transform_input:\n            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n        return x\n\n    def _forward(self, x: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        x = self.maxpool1(x)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        x = self.maxpool2(x)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        aux: Optional[Tensor] = None\n        if self.AuxLogits is not None:\n            if self.training:\n                aux = self.AuxLogits(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        # Adaptive average pooling\n        x = self.avgpool(x)\n        # N x 2048 x 1 x 1\n        x = self.dropout(x)\n        # N x 2048 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 2048\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        return x, aux\n\n    @torch.jit.unused\n    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]) -> InceptionOutputs:\n        if self.training and self.aux_logits:\n            return InceptionOutputs(x, aux)\n        else:\n            return x  # type: ignore[return-value]\n\n    def forward(self, x: Tensor) -> InceptionOutputs:\n        x = self._transform_input(x)\n        x, aux = self._forward(x)\n        aux_defined = self.training and self.aux_logits\n        if torch.jit.is_scripting():\n            if not aux_defined:\n                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n            return InceptionOutputs(x, aux)\n        else:\n            return self.eager_outputs(x, aux)\n\n\nclass InceptionA(nn.Module):\n    def __init__(\n        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n\n    def _forward(self, x: Tensor) -> List[Tensor]:\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x: Tensor) -> Tensor:\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n\n    def _forward(self, x: Tensor) -> List[Tensor]:\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x: Tensor) -> Tensor:\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n    def __init__(\n        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x: Tensor) -> List[Tensor]:\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return outputs\n\n    def forward(self, x: Tensor) -> Tensor:\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n\n    def _forward(self, x: Tensor) -> List[Tensor]:\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return outputs\n\n    def forward(self, x: Tensor) -> Tensor:\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x: Tensor) -> List[Tensor]:\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x: Tensor) -> Tensor:\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n    def __init__(\n        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super().__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n        #err1 5 na 3\n        self.conv1 = conv_block(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01  # type: ignore[assignment]\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001  # type: ignore[assignment]\n\n    def forward(self, x: Tensor) -> Tensor:\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:37:03.567582Z","iopub.execute_input":"2022-12-15T11:37:03.567974Z","iopub.status.idle":"2022-12-15T11:37:03.640984Z","shell.execute_reply.started":"2022-12-15T11:37:03.567938Z","shell.execute_reply":"2022-12-15T11:37:03.640021Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = MyInception_v3(num_classes=n_classes)\n#model = Inception3(num_classes=n_classes)\n#from torchvision.models import Inception3\n#model = Inception3()\n#model.fc = nn.Linear(2048, n_classes)\n#torch.nn.init.xavier_uniform(model.fc.weight)\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).to(device)\n#model.aux_logits = False\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:37:34.040260Z","iopub.execute_input":"2022-12-15T11:37:34.040655Z","iopub.status.idle":"2022-12-15T11:37:43.595813Z","shell.execute_reply.started":"2022-12-15T11:37:34.040623Z","shell.execute_reply":"2022-12-15T11:37:43.594596Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/104M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"497dca8e19d34b09ad18e19ba0adb327"}},"metadata":{}}]},{"cell_type":"code","source":"def return_params(model):\n    return list(model.parameters())\n\ndef state_params(model, new_params):\n    params = model.state_dict()\n    params = new_params","metadata":{"execution":{"iopub.status.busy":"2022-12-13T21:53:38.267466Z","iopub.execute_input":"2022-12-13T21:53:38.267869Z","iopub.status.idle":"2022-12-13T21:53:38.275163Z","shell.execute_reply.started":"2022-12-13T21:53:38.267833Z","shell.execute_reply":"2022-12-13T21:53:38.274189Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"\ncriterion = torch.nn.CrossEntropyLoss()\n#optimizer = AdaSmooth(model, return_params, state_params)\noptimizer = AdaSmooth(model.parameters(), lr=1e-2, M = len(dataloaders['train']))\n#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\nf1 = F1Score(num_classes=n_classes, average = 'macro')","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:38:08.785817Z","iopub.execute_input":"2022-12-15T11:38:08.786213Z","iopub.status.idle":"2022-12-15T11:38:08.798411Z","shell.execute_reply.started":"2022-12-15T11:38:08.786179Z","shell.execute_reply":"2022-12-15T11:38:08.796959Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=3):\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n        eps = 1e-6\n        for phase in ['train']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n            all_true_labels = []\n            all_preds = []\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                \n                \n                labels = labels.to(device)\n                \n                outputs,aux_outputs = model(inputs)\n                loss1 = criterion(outputs, labels)\n                loss2 = criterion(aux_outputs, labels)\n                loss = loss1 + 0.4 * loss2\n                #loss = loss1\n                #print(loss1)\n                #print(loss2)\n                #if loss.isnan():\n                #    loss = eps\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    #torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n                    #model.float()\n                    optimizer.step()\n\n                _, preds = torch.max(outputs, 1)\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n                all_true_labels.extend(labels.tolist())\n                all_preds.extend(preds.tolist())\n\n            epoch_loss = running_loss / len(datasets[phase])\n            epoch_acc = running_corrects.double() / len(datasets[phase])\n            \n            #epoch_f_score = f1(torch.tensor(all_true_labels), torch.tensor(all_preds))\n\n            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n                                                        epoch_loss,\n                                                        epoch_acc\n                                                        ))\n            # print(f\"{phase} loss: {epoch_loss:.4f}, f_score: {epoch_f_score:.4f}, accuracy: {epoch_acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:38:12.121362Z","iopub.execute_input":"2022-12-15T11:38:12.121709Z","iopub.status.idle":"2022-12-15T11:38:12.133922Z","shell.execute_reply.started":"2022-12-15T11:38:12.121679Z","shell.execute_reply":"2022-12-15T11:38:12.132743Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#запуск с адабустом\ntrain_model(model, criterion, optimizer, 15)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T11:38:15.276631Z","iopub.execute_input":"2022-12-15T11:38:15.277019Z","iopub.status.idle":"2022-12-15T12:49:55.934026Z","shell.execute_reply.started":"2022-12-15T11:38:15.276987Z","shell.execute_reply":"2022-12-15T12:49:55.933109Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [06:04<00:00,  1.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.5517, acc: 0.0063\nEpoch 2/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:40<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4780, acc: 0.0033\nEpoch 3/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:40<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4937, acc: 0.0048\nEpoch 4/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:39<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4814, acc: 0.0050\nEpoch 5/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:39<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4727, acc: 0.0063\nEpoch 6/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:41<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4867, acc: 0.0050\nEpoch 7/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:39<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.4374, acc: 0.0053\nEpoch 8/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:40<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.2350, acc: 0.0056\nEpoch 9/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:39<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.1987, acc: 0.0050\nEpoch 10/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:40<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.2434, acc: 0.0050\nEpoch 11/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:42<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.2838, acc: 0.0055\nEpoch 12/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:44<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.3084, acc: 0.0056\nEpoch 13/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:45<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.3130, acc: 0.0066\nEpoch 14/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:42<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 9.3914, acc: 0.0052\nEpoch 15/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:39<00:00,  1.82it/s]","output_type":"stream"},{"name":"stdout","text":"train loss: 9.3928, acc: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# оптимищация с помощью адама\ntrain_model(model, criterion, optimizer, 15)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T14:11:03.491697Z","iopub.execute_input":"2022-12-14T14:11:03.492072Z","iopub.status.idle":"2022-12-14T15:14:35.574519Z","shell.execute_reply.started":"2022-12-14T14:11:03.492031Z","shell.execute_reply":"2022-12-14T15:14:35.573428Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [05:18<00:00,  1.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 8.0561, acc: 0.0048\nEpoch 2/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:08<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.4927, acc: 0.0048\nEpoch 3/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:08<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.4449, acc: 0.0052\nEpoch 4/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:09<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.4096, acc: 0.0050\nEpoch 5/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:06<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.3874, acc: 0.0061\nEpoch 6/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:08<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.3580, acc: 0.0061\nEpoch 7/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:08<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.3322, acc: 0.0070\nEpoch 8/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:07<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.3085, acc: 0.0070\nEpoch 9/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:09<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.2625, acc: 0.0101\nEpoch 10/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:08<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.2231, acc: 0.0090\nEpoch 11/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:09<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.1660, acc: 0.0113\nEpoch 12/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:07<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.1193, acc: 0.0104\nEpoch 13/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:14<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.0674, acc: 0.0108\nEpoch 14/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:13<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"train loss: 7.0011, acc: 0.0101\nEpoch 15/15\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 509/509 [04:12<00:00,  2.01it/s]","output_type":"stream"},{"name":"stdout","text":"train loss: 6.8524, acc: 0.0142\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}