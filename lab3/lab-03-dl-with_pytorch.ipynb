{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ipymarkup","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:48:42.153599Z","iopub.execute_input":"2023-01-18T12:48:42.154111Z","iopub.status.idle":"2023-01-18T12:48:58.730653Z","shell.execute_reply.started":"2023-01-18T12:48:42.154017Z","shell.execute_reply":"2023-01-18T12:48:58.728718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch\nfrom torch import nn\nimport re\nfrom tqdm import tqdm, tqdm_notebook\nfrom torch.optim import Adam, AdamW\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\npd.options.mode.chained_assignment = None  # default='warn'\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom ipymarkup import show_span_box_markup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T12:48:58.733608Z","iopub.execute_input":"2023-01-18T12:48:58.734059Z","iopub.status.idle":"2023-01-18T12:49:09.368227Z","shell.execute_reply.started":"2023-01-18T12:48:58.734017Z","shell.execute_reply":"2023-01-18T12:49:09.366685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Для версии Андрея\n# PATH1 = '/kaggle/input/rurebustraindata/train_part_1/train_part_1'\n# PATH2 = '/kaggle/input/rurebustraindata/train_part_2/train_part_2'\n# PATH3 = '/kaggle/input/rurebustraindata/train_part_3/train_part_3'\n# Для версии Гарри и Тани\nPATH1 = '/kaggle/input/rurebus-train-data/train_part_1/train_part_1'\nPATH2 = '/kaggle/input/rurebus-train-data/train_part_2/train_part_2'\nPATH3 = '/kaggle/input/rurebus-train-data/train_part_3/train_part_3'\nPATH4 = '/kaggle/input/rurebus-train-data/test_ner_only' #чтобы все ок было надо обновить датасет до последней версии\n# имена сущностей\nNAMES = ['BIN', 'SOC', 'MET', 'CMP', 'ECO', 'INST', 'ACT', 'QUA']\n# тэги для разметки\nTAG = ['B-bin', 'I-bin', 'B-soc','I-soc','B-met', 'I-met','B-cmp','I-cmp','B-eco', 'I-eco','B-inst','I-inst','B-act','I-act','B-qua','I-qua']","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:49:09.369882Z","iopub.execute_input":"2023-01-18T12:49:09.370563Z","iopub.status.idle":"2023-01-18T12:49:09.380521Z","shell.execute_reply.started":"2023-01-18T12:49:09.370525Z","shell.execute_reply":"2023-01-18T12:49:09.378148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В тесте присутствует множество тестов разбитых по частям (part_1, part_2 и тд.). Они представляют собой логически целое однако среди нет переносов в аннотациях. \nВ аннотации присутствует следующая информация: именование сущности, ее класс а также местоположение в тексте (посимвольное).\nЧто касается отношений то дается информация о номере сущности и классе связи.\nДля дальнейшем работы необходимо понять какой формат входных данных нужен для cnn lstm \n","metadata":{}},{"cell_type":"code","source":"# подготовка данных для обучения\ndef make_data(lst,path):        \n    res_df = pd.DataFrame(columns = ['word', 'tag'])\n    with tqdm(desc=\"n\", total=len(lst)) as pbar_outer:\n        for item in lst:\n            df1 = make_ann(item,path)\n            df2 = make_text(item, df1,path)\n            res_df = pd.concat([res_df, df2], ignore_index=True)\n            pbar_outer.update(1)\n    return res_df\n\n\n# продготовка аннотаций\ndef make_ann(file,path):\n    # открываю файл\n    ann_df = pd.read_csv(path+'/'+file+'.ann', sep='\\t', engine='python', header=None, on_bad_lines='skip') # здесь добавлено скипанье плохих строк(если например столбцов 6, а в трок только 4), наверно это не оч хорошо надо что-то придумать\n    # нормальные названия для столбцов\n    ann_df.rename(columns = {1:'class', 2:'words'}, inplace = True )\n    # разделяю в разные столбцы классы и координаты\n    ann_df.insert(2, 'coords' , ann_df['class'])\n    ann_df['class'] = ann_df['class'].apply(lambda x: x.split(\" \")[0])\n    ann_df['coords'] = ann_df['coords'].apply(lambda x: x.split(\" \")[1:])\n    # удаляю нафиг строки с отношениями\n    ann_df = ann_df.dropna()\n    ann_df.reset_index(drop= True , inplace= True )\n    # для удобства\n    ann_df.insert(2, 'coords1' , ann_df['coords'])\n    ann_df['coords1'] = ann_df['coords1'].apply(lambda x: int(x[0]))\n    ann_df['words'] = ann_df['words'].apply(lambda x: my_split(x.split(\" \")))\n    ann_df['words'] = ann_df['words'].apply(lambda x: del_all(x))\n    ann_df['words'] = ann_df['words'].apply(lambda x: [item.strip() for item in x if item not in ['','»', '«',':',' ']])\n    ann_df = ann_df.sort_values(by='coords1')\n    ann_df.reset_index(drop= True , inplace= True )\n    return ann_df\n\n\n# разметка\ndef make_text(file, df,path):\n    # открываем файл и записываем его в dataframe\n    with open(path+'/'+file+'.txt') as f:\n        lines = f.readlines()\n    text_df = pd.DataFrame({'word':lines})\n    # считаем длины строк для удобства дальнейшей разметки\n    text_df.insert(1, 'len' , text_df['word'].copy())\n    text_df['len'] = text_df['len'].apply(lambda x: len(x)) # считаем длину строки\n    new_lens = [text_df['len'][0]] # считаем длину предыдущих строк + длина новой строки\n    for i in range(1, len(list(text_df['len']))):\n        new_lens.append(sum(list(text_df['len'])[:i+1]))\n    text_df.insert(2, 'new_len' , new_lens)\n    # добавляем столбец для разметки\n    text_df.insert(3, 'tag', 0)\n    # удаляем \\n\n    text_df['word'] = text_df['word'].apply(lambda x: re.split('\\n',x)[0])\n    #удаляем строрки с []\n    idx = [i for i in range(len(text_df)) if len(text_df['word'][i])==0]\n    text_df = text_df.drop(index=idx)\n    text_df.reset_index(drop = True, inplace= True)\n    # делаем разметку\n    # находим индекс строки для каждой аннотации\n    df = find_rows(text_df, df)\n#     print(df.head(5))\n    # преобразовываем строку в массив\n    text_df['word'] = text_df['word'].apply(lambda x: my_split([item for item in re.split(' ',x) if item != '']))\n    text_df['word'] = text_df['word'].apply(lambda x: del_all(x))\n    text_df['word'] = text_df['word'].apply(lambda x: [item.strip() for item in x if item not in ['',' ']])\n    # момент разметки\n    text_df = make_markup(text_df, df)\n    text_df = last_changes(text_df)\n#     # удаляем лишнее\n    del text_df['len']\n    del text_df['new_len']\n    text_df['word'] = text_df['word'].apply(lambda x: [item.lower() for item in x if item not in ['',' ']])\n    idx = [i for i in range(len(text_df)) if len(text_df['word'][i])==0]\n    text_df = text_df.drop(index=idx)\n    text_df.reset_index(drop = True, inplace= True)\n    return text_df\n\n\n# отделяем все что можно\ndef my_split(lst):\n    for k in range(3):\n        for i in range(len(lst)-1,-1,-1):\n            if lst[i] in ['',' ']:\n                lst.pop(i)\n            else:\n                lst[i] = lst[i].replace('\\xa0', ' ')\n                lst[i] = lst[i].replace('\\t', ' ')\n                lst[i]=lst[i].replace('………………','')\n                lst[i]=lst[i].replace('………','')\n                lst[i] = lst[i].replace('……','')\n                idx = []\n                lens = len(lst[i])\n                for item in ['+', ')', '»',';','.',',', '\"','(', '«',':',' ', '-\\t','\\\\', '/','”','“','-','–','_________','*','№','%']:\n                     idx.append(lst[i].find(item))\n                for item in ['\\xa0','\"','.']:\n                    if lst[i].endswith(item) and lens-1 not in idx:\n                        idx.append(lens-1)                    \n                idx.sort(reverse=True)\n                for item in idx:\n                    if item!=-1 and item!=0:\n                        lst.insert(i+1, lst[i][item])\n                        lst.insert(i+2, lst[i][item+1:])\n                        lst[i] = lst[i][:item]\n                    elif item!=-1 and item == 0:\n                        lst.insert(i+1, lst[i][item+1:])\n                        lst[i] = lst[i][item]\n                    elif item!= -1 and item == lens-1:\n                        lst.insert(i+1, lst[i][item])\n                        lst[i] = lst[i][:item] \n    return lst\n\n\ndef del_all(lst):\n    for i in range(len(lst)-1,-1,-1):\n        split = lst[i]\n        split = split.split(' ')\n        if len(split) >1:\n            for j in range(len(split)):\n                lst.insert(i+1+j, split[j])\n            lst.pop(i)\n    return lst\n\n\n# обработка строк без сущностей и оставшихся слов\ndef last_changes(df):\n    for i in range(len(df)):\n        if df['tag'][i] == 0:\n            df['tag'][i] = ['O'] * len(df['word'][i])\n        else:\n#             df['tag'][i] = my_split(df['tag'][i])\n            llst = []\n            for item in df['tag'][i]:\n                if item in TAG:\n                    llst.append(item)\n                else:\n                    llst.append('O')\n            df['tag'][i] = llst\n        assert (len(df['word'][i]) == len(df['tag'][i]))\n    return df\n\n# находим индекс строки для каждой аннотации\ndef find_rows(txt_df, ann_df):\n    ann_df.insert(4, 'idx' , 0)\n    for i in range(len(ann_df)):\n        for j in range(len(txt_df)):\n            start = txt_df['new_len'][j]-txt_df['len'][j]\n            end = txt_df['new_len'][j]\n            if int(ann_df['coords'][i][0]) in np.arange(start, end):\n                ann_df['idx'][i] = j\n                break\n            else:\n                pass\n    return ann_df\n    \n# основная часть разметки\ndef make_markup(text_df, ann_df):\n    for i in range(len(ann_df)):\n        words_count = len(ann_df['words'][i])\n        lens = [len(item) for item in ann_df['words'][i]] \n        row = text_df.iloc[[ann_df['idx'][i]]]\n#         print(row['word'].item())\n        if row['tag'].item() == 0:\n                        text_df['tag'][ann_df['idx'][i]] = text_df['word'][ann_df['idx'][i]].copy()\n                        row['tag'] = row['word']\n        for k in range(words_count):\n                if k == 0:\n                    idx = (row['tag'].item()).index(ann_df['words'][i][k])\n                    text_df['tag'][ann_df['idx'][i]][idx] = 'B-'+ ann_df['class'][i].lower()\n                else:\n                    idx = row['tag'].item().index(ann_df['words'][i][k])\n                    text_df['tag'][ann_df['idx'][i]][idx] = 'I-'+ ann_df['class'][i].lower()\n    return text_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:49:09.384375Z","iopub.execute_input":"2023-01-18T12:49:09.385617Z","iopub.status.idle":"2023-01-18T12:49:09.435953Z","shell.execute_reply.started":"2023-01-18T12:49:09.385545Z","shell.execute_reply":"2023-01-18T12:49:09.434343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# список всех файлов для обучения\ndata_lst = list(set([item[:-4] for item in os.listdir(PATH1)]))\ndata_lst.remove('.stats_c')\ntrain_df = make_data(data_lst, PATH1)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:49:09.437668Z","iopub.execute_input":"2023-01-18T12:49:09.438120Z","iopub.status.idle":"2023-01-18T12:50:19.491782Z","shell.execute_reply.started":"2023-01-18T12:49:09.438068Z","shell.execute_reply":"2023-01-18T12:50:19.490646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#файлы для валидации\ndata_lst = list(set([item[:-4] for item in os.listdir(PATH2)]))\ndata_lst = data_lst[0]\nval_df = make_data([data_lst], PATH2)\nval_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:19.493745Z","iopub.execute_input":"2023-01-18T12:50:19.494266Z","iopub.status.idle":"2023-01-18T12:50:19.728318Z","shell.execute_reply.started":"2023-01-18T12:50:19.494234Z","shell.execute_reply":"2023-01-18T12:50:19.727003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab_from_iterator(train_df['word'], min_freq=1, specials=[\"<UNK>\"])\nvocab.set_default_index(vocab[\"<UNK>\"])\nvocab_lables = build_vocab_from_iterator(train_df['tag'], min_freq=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:19.730130Z","iopub.execute_input":"2023-01-18T12:50:19.730571Z","iopub.status.idle":"2023-01-18T12:50:20.058580Z","shell.execute_reply.started":"2023-01-18T12:50:19.730527Z","shell.execute_reply":"2023-01-18T12:50:20.057162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab(train_df['word'][0])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:20.060128Z","iopub.execute_input":"2023-01-18T12:50:20.060522Z","iopub.status.idle":"2023-01-18T12:50:20.069029Z","shell.execute_reply.started":"2023-01-18T12:50:20.060487Z","shell.execute_reply":"2023-01-18T12:50:20.067429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_tl(df):\n    tokens = []\n    lables = []\n    max_len = 0\n    for ind in df.index:\n        tokens.append(vocab(df['word'][ind]))\n        lables.append(vocab_lables(df['tag'][ind]))\n        if len(df['word'][ind]) > max_len:\n            max_len = len(df['word'][ind])\n    df['tokens'] = tokens\n    df['lables'] = lables\n    return df,max_len\n\n\ndef make_pad(df):\n    list_sent = []\n    list_labels = []\n    for ind in df.index:\n        list_sent.append(df['tokens'][ind])\n        list_labels.append(df['lables'][ind])\n    padded_sent = pad_sequences(list_sent)\n    padded_labels = pad_sequences(list_labels)\n    print(padded_sent.shape)\n    padd_df = pd.DataFrame(columns = ['sentence', 'labels'])\n    padd_df['sentence'] = pd.Series(padded_sent.tolist())\n    padd_df['labels'] = pd.Series(padded_labels.tolist())\n    return padd_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:20.071302Z","iopub.execute_input":"2023-01-18T12:50:20.071863Z","iopub.status.idle":"2023-01-18T12:50:20.083479Z","shell.execute_reply.started":"2023-01-18T12:50:20.071812Z","shell.execute_reply":"2023-01-18T12:50:20.081990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df,max_len1 = make_tl(train_df)\nval_df,max_len2 = make_tl(val_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:20.087282Z","iopub.execute_input":"2023-01-18T12:50:20.087684Z","iopub.status.idle":"2023-01-18T12:50:20.399816Z","shell.execute_reply.started":"2023-01-18T12:50:20.087637Z","shell.execute_reply":"2023-01-18T12:50:20.398460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(max_len1, ' ',max_len2)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:20.401122Z","iopub.execute_input":"2023-01-18T12:50:20.401470Z","iopub.status.idle":"2023-01-18T12:50:20.407880Z","shell.execute_reply.started":"2023-01-18T12:50:20.401439Z","shell.execute_reply":"2023-01-18T12:50:20.406668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# делаем паддинг \ntrain_df = make_pad(train_df)\nval_df = make_pad(val_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:20.409403Z","iopub.execute_input":"2023-01-18T12:50:20.409853Z","iopub.status.idle":"2023-01-18T12:50:21.033555Z","shell.execute_reply.started":"2023-01-18T12:50:20.409785Z","shell.execute_reply":"2023-01-18T12:50:21.032181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_df['sentence'][4]),len(val_df['sentence'][4]))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.035857Z","iopub.execute_input":"2023-01-18T12:50:21.036346Z","iopub.status.idle":"2023-01-18T12:50:21.044129Z","shell.execute_reply.started":"2023-01-18T12:50:21.036299Z","shell.execute_reply":"2023-01-18T12:50:21.042717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(x: np.ndarray, vocab_len: int) -> np.ndarray:\n    \"\"\"\n    Args:\n        x - одномерный массив значений словаря\n        vocab_len - длина словаря\n    Выход:\n        двумерный массив encoded, где encoded[i] - результат one hot кодирования x[i]\n    \"\"\"\n    encoded = np.zeros((len(x), vocab_len))\n    for i in range(len(x)):\n        encoded[i][x[i]] = 1\n    return encoded\n\ntext_vocab_len = len(vocab)\ntarget_vocab_len = len(vocab_lables)\n\nclass TokenDataset(torch.utils.data.Dataset):\n    def __init__(self, data, text_vocab_len = text_vocab_len, target_vocab_len = target_vocab_len, classes = None,\n                 transform=None, target_transform=None):\n        self.data = data\n        self.sequence_len = len(data.iloc[0][0])\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        tokens, tag = self.data.iloc[idx]\n        return torch.Tensor(tokens).int(), torch.Tensor(one_hot(tag, target_vocab_len))\n        # return torch.Tensor(tokens).int(), torch.Tensor(tag)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.045982Z","iopub.execute_input":"2023-01-18T12:50:21.046531Z","iopub.status.idle":"2023-01-18T12:50:21.058980Z","shell.execute_reply.started":"2023-01-18T12:50:21.046482Z","shell.execute_reply":"2023-01-18T12:50:21.057812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasets = {\n    'train': TokenDataset(train_df),\n    'val': TokenDataset(val_df)\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.061038Z","iopub.execute_input":"2023-01-18T12:50:21.061512Z","iopub.status.idle":"2023-01-18T12:50:21.080897Z","shell.execute_reply.started":"2023-01-18T12:50:21.061466Z","shell.execute_reply":"2023-01-18T12:50:21.079288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = {\n    'train':\n    torch.utils.data.DataLoader(datasets['train'],\n                                batch_size=16,\n                                shuffle=True,\n                                num_workers=0),  # for Kaggle\n    'val':\n    torch.utils.data.DataLoader(datasets['val'],\n                                batch_size=16,\n                                shuffle=False,\n                                num_workers=0)  # for Kaggle\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.082756Z","iopub.execute_input":"2023-01-18T12:50:21.083293Z","iopub.status.idle":"2023-01-18T12:50:21.095174Z","shell.execute_reply.started":"2023-01-18T12:50:21.083244Z","shell.execute_reply":"2023-01-18T12:50:21.093879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.097038Z","iopub.execute_input":"2023-01-18T12:50:21.097667Z","iopub.status.idle":"2023-01-18T12:50:21.110190Z","shell.execute_reply.started":"2023-01-18T12:50:21.097628Z","shell.execute_reply":"2023-01-18T12:50:21.109163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nclass CustomLSTM(nn.Module):\n    def __init__(self, input_sz, hidden_sz):\n        super().__init__()\n        self.input_sz = input_sz\n        self.hidden_size = hidden_sz\n        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n        self.init_weights()\n                \n    def init_weights(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n         \n    def forward(self, x, \n                init_states=None):\n        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n        bs, seq_sz, _ = x.size()\n        hidden_seq = []\n        if init_states is None:\n            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n                        torch.zeros(bs, self.hidden_size).to(x.device))\n        else:\n            h_t, c_t = init_states\n         \n        HS = self.hidden_size\n        for t in range(seq_sz):\n            x_t = x[:, t, :]\n            # batch the computations into a single matrix multiplication\n            gates = x_t @ self.W + h_t @ self.U + self.bias\n            i_t, f_t, g_t, o_t = (\n                torch.sigmoid(gates[:, :HS]), # input\n                torch.sigmoid(gates[:, HS:HS*2]), # forget\n                torch.tanh(gates[:, HS*2:HS*3]),\n                torch.sigmoid(gates[:, HS*3:]), # output\n            )\n            c_t = f_t * c_t + i_t * g_t\n            h_t = o_t * torch.tanh(c_t)\n            hidden_seq.append(h_t.unsqueeze(0))\n        hidden_seq = torch.cat(hidden_seq, dim=0)\n        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n        return hidden_seq, (h_t, c_t)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:07:06.827501Z","iopub.execute_input":"2023-01-18T13:07:06.827991Z","iopub.status.idle":"2023-01-18T13:07:06.843491Z","shell.execute_reply.started":"2023-01-18T13:07:06.827950Z","shell.execute_reply":"2023-01-18T13:07:06.842084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_LSTM(nn.Module):\n    def __init__(self, vocab_size, n_classes, embedding_dim=250, hidden_size = 32, filters=((2, 10), (3, 8))):\n        super().__init__()\n        \n        self.embeddings_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=200, kernel_size=3,padding=1)\n        self.pool1 = nn.MaxPool1d(2)\n        input_size = 100\n        self.hidden_size = hidden_size\n        self.lstm_layer = CustomLSTM(input_size, hidden_size)\n        self.fc = nn.Linear(self.hidden_size, n_classes)\n        outputs = []\n\n    def forward(self, inputs):\n        projections = self.embeddings_layer.forward(inputs) \n        projections = projections.transpose(1, 2)\n        projections = self.conv1(projections)\n        projections = projections.transpose(1, 2)\n        projections = self.pool1(projections)\n        output, (final_hidden_state, final_cell_state) = self.lstm_layer(projections)\n        output = output.reshape(-1, self.hidden_size)\n        output = self.fc(output)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.128729Z","iopub.execute_input":"2023-01-18T12:50:21.129384Z","iopub.status.idle":"2023-01-18T12:50:21.148577Z","shell.execute_reply.started":"2023-01-18T12:50:21.129347Z","shell.execute_reply":"2023-01-18T12:50:21.146814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, num_epochs=3):\n    all_true_labels = []\n    all_preds = []\n    inputs_str = []\n    loss_list = {'train' : [], 'val':[]}\n    acc_list = {'train' : [], 'val':[]}\n    f1_list = {'train' : [], 'val':[]}\n#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n#                                                    step_size=10,\n#                                                    gamma=0.2)\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                print('start train')\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            all_true_labels = []\n            all_preds = []\n            inputs_str = []\n            for inputs, labels in tqdm(dataloader[phase]):\n                batch_size, n_words, n_classes = labels.shape\n                labels = labels.reshape(-1, n_classes).to(device)\n                outputs = model(inputs.to(device))\n                \n                loss = criterion(outputs, labels)\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                _, preds = torch.max(outputs, 1)\n                running_loss += loss.item() #* inputs.size(0)\n                _, labels = torch.max(labels, 1)\n\n                all_true_labels.extend(labels.tolist())\n                all_preds.extend(preds.tolist())\n            epoch_loss = running_loss /len(dataloader[phase])\n            epoch_acc = accuracy_score(all_preds, all_true_labels)\n            epoch_f1 = f1_score(all_preds, all_true_labels, average='macro')\n#             lr_scheduler.step()  \n            print('{} loss: {:.4f}, acc: {:.4f}, f1: {:.4f}'.format(phase,\n                                                        epoch_loss,\n                                                        epoch_acc,\n                                                        epoch_f1            \n                                                        ))\n            loss_list[phase].append(epoch_loss)\n            acc_list[phase].append(epoch_acc.tolist())\n            f1_list[phase].append(epoch_f1.tolist())\n    return all_true_labels, all_preds, inputs_str, loss_list, acc_list, f1_list","metadata":{"execution":{"iopub.status.busy":"2023-01-18T12:50:21.151138Z","iopub.execute_input":"2023-01-18T12:50:21.151654Z","iopub.status.idle":"2023-01-18T12:50:21.169327Z","shell.execute_reply.started":"2023-01-18T12:50:21.151606Z","shell.execute_reply":"2023-01-18T12:50:21.167859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(vocab_lables)\nmodel = CNN_LSTM(len(vocab), n_classes = n_classes).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = Adam(model.parameters(), lr = 3e-4)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:07:11.816902Z","iopub.execute_input":"2023-01-18T13:07:11.817329Z","iopub.status.idle":"2023-01-18T13:07:11.868708Z","shell.execute_reply.started":"2023-01-18T13:07:11.817296Z","shell.execute_reply":"2023-01-18T13:07:11.867366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epox_num = 15\nall_true_labels, all_preds, inputs_str, loss, acc, f1 = train_model(model, criterion, optimizer, epox_num)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:07:14.504839Z","iopub.execute_input":"2023-01-18T13:07:14.505263Z","iopub.status.idle":"2023-01-18T13:58:15.536096Z","shell.execute_reply.started":"2023-01-18T13:07:14.505229Z","shell.execute_reply":"2023-01-18T13:58:15.534890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epox_list = [i for i in range(epox_num)]\ndef graf(loss, acc):\n    \n    fig, ax = plt.subplots(2, 3, figsize=(26, 13))\n    ax[0, 0].plot(epox_list, loss['train'])\n    ax[0, 0].set_title(\"Изменение потерь на обучающей выборке\")\n    ax[0, 1].plot(epox_list, acc['train'])\n    ax[0, 1].set_title(\"Изменение точности на обучающей выборке\")\n    ax[0, 2].plot(epox_list, f1['train'])\n    ax[0, 2].set_title(\"Изменение f1-score на обучающей выборке\")\n    ax[1, 0].plot(epox_list, loss['val'])\n    ax[1, 0].set_title(\"Изменение потерь на валидационной выборке\")\n    ax[1, 1].plot(epox_list, acc['val'])\n    ax[1, 1].set_title(\"Изменение точности на валидационной выборке\")\n    ax[1, 2].plot(epox_list, f1['val'])\n    ax[1, 2].set_title(\"Изменение f1-score на валидационной выборке\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:58:15.538921Z","iopub.execute_input":"2023-01-18T13:58:15.539728Z","iopub.status.idle":"2023-01-18T13:58:15.550474Z","shell.execute_reply.started":"2023-01-18T13:58:15.539680Z","shell.execute_reply":"2023-01-18T13:58:15.549528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ngraf(loss, acc)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:58:15.552271Z","iopub.execute_input":"2023-01-18T13:58:15.553105Z","iopub.status.idle":"2023-01-18T13:58:16.584612Z","shell.execute_reply.started":"2023-01-18T13:58:15.553059Z","shell.execute_reply":"2023-01-18T13:58:16.583366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test(model,dataloader,dataset):\n    all_true_labels = []\n    all_preds = []\n    inputs_str = []\n    acc_list = []\n    model.eval()\n    for inputs, labels in tqdm(dataloader):\n        batch_size, n_words, n_classes = labels.shape\n        labels = labels.reshape(-1, n_classes).to(device)\n        outputs = model(inputs.to(device))\n        _, preds = torch.max(outputs, 1)\n        _, labels = torch.max(labels, 1)\n        all_true_labels.extend(labels.tolist())\n        all_preds.extend(preds.tolist())\n    epoch_acc = accuracy_score(all_preds, all_true_labels)\n    epoch_f1 = f1_score(all_preds, all_true_labels, average='macro')\n    print('acc: {:.4f}, f1: {:.4f}'.format(epoch_acc, epoch_f1))\n    acc_list.append(epoch_acc.tolist())\n    return all_true_labels,all_preds","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:58:16.587466Z","iopub.execute_input":"2023-01-18T13:58:16.588745Z","iopub.status.idle":"2023-01-18T13:58:16.599631Z","shell.execute_reply.started":"2023-01-18T13:58:16.588689Z","shell.execute_reply":"2023-01-18T13:58:16.598210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_lst = list(set([item[:-4] for item in os.listdir(PATH4)]))\ndata_lst.remove('31339221025603182330049_24_part_1')\ndata_lst.remove('31339131024502051716072_24_part_1')\ndata_lst.remove('31339251033301001216016_20_part_2')\ndata_lst.remove('31339011021101006981035_9_part_1')\ndata_lst.remove('31339011021100987258005_5_part_2')\ndata_lst.remove('31339011061672000026002_4_part_2')\ndata_lst.remove('31339011026200597103018_8_part_1')\ndata_lst.remove('31339201027002952877006_14_part_2')\ntest_df = make_data(data_lst[:100], PATH4)\ntest_df,max_len1 = make_tl(test_df)\ntest_df = make_pad(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:58:16.601216Z","iopub.execute_input":"2023-01-18T13:58:16.601596Z","iopub.status.idle":"2023-01-18T13:59:08.533213Z","shell.execute_reply.started":"2023-01-18T13:58:16.601562Z","shell.execute_reply":"2023-01-18T13:59:08.532176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset =  TokenDataset(test_df)\ntest_dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0)\nall_t,all_p = make_test(model,test_dataloader,dataset)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T13:59:08.534697Z","iopub.execute_input":"2023-01-18T13:59:08.535104Z","iopub.status.idle":"2023-01-18T14:03:45.949230Z","shell.execute_reply.started":"2023-01-18T13:59:08.535069Z","shell.execute_reply":"2023-01-18T14:03:45.947890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# убираем палдинги \ndata = {'sentence': test_df['sentence'], 'real_cat': np.array_split(all_t, len(test_df['sentence'])), 'pred_cat':np.array_split(all_p, len(test_df['sentence']))}\ndf_test_see = pd.DataFrame(data)\nfor ind in df_test_see.index:\n    sent = df_test_see['sentence'][ind]\n    try:\n        len_pad = sent.index(next(filter(lambda x: x!=0, sent)))\n    except StopIteration:\n        pass\n    sent = sent[len_pad:]\n    sent = vocab.lookup_tokens(sent)\n    df_test_see.at[ind, 'sentence'] = sent \n    sent = df_test_see['real_cat'][ind]\n    sent = sent[len_pad:]\n    sent = vocab_lables.lookup_tokens(sent)\n    df_test_see.at[ind, 'real_cat'] = sent \n    sent = df_test_see['pred_cat'][ind]\n    sent = sent[len_pad:]\n    sent = vocab_lables.lookup_tokens(sent)\n    df_test_see.at[ind, 'pred_cat'] = sent \ndf_test_see","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:03:45.951303Z","iopub.execute_input":"2023-01-18T14:03:45.951810Z","iopub.status.idle":"2023-01-18T14:03:47.127685Z","shell.execute_reply.started":"2023-01-18T14:03:45.951743Z","shell.execute_reply":"2023-01-18T14:03:47.126614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_show(row,lbl):\n    spans = []\n    coords = 0\n    c1 = 0\n    c2 = 0\n    tag = ''\n    text = ''\n    last = 0\n    for i in range(len(row)):\n        text += row[i]+' '\n        if lbl[i] in TAG:\n            if 'B-' in lbl[i]:\n                c1 = coords\n                c2 = coords + len(row[i])\n                tag = lbl[i][2:].upper()\n                coords+=1+len(row[i])\n            elif 'I-' in lbl[i]:\n                c2+=1+len(row[i])\n                coords+=1+len(row[i])\n        elif lbl[i] =='O':\n            coords += len(row[i])+1\n        if (c1,c2,tag) !=(0, 0, ''):\n            if last == c1:\n                spans = spans[:-1]\n            spans.append((c1,c2,tag))\n            last = c1\n    return text, spans","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:03:47.129428Z","iopub.execute_input":"2023-01-18T14:03:47.130422Z","iopub.status.idle":"2023-01-18T14:03:47.140954Z","shell.execute_reply.started":"2023-01-18T14:03:47.130376Z","shell.execute_reply":"2023-01-18T14:03:47.139652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_idx = 5971\nrow = df_test_see['sentence'][row_idx]\ntrue_lbl = df_test_see['real_cat'][row_idx]\npred_lbl = df_test_see['pred_cat'][row_idx]\ntext,spans1 = make_show(row,true_lbl)\n_,spans2 = make_show(row,pred_lbl)\nprint('true:')\nshow_span_box_markup(text, spans1)\nprint('pred:')\nshow_span_box_markup(text, spans2)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:03:47.143256Z","iopub.execute_input":"2023-01-18T14:03:47.143718Z","iopub.status.idle":"2023-01-18T14:03:47.162475Z","shell.execute_reply.started":"2023-01-18T14:03:47.143683Z","shell.execute_reply":"2023-01-18T14:03:47.161220Z"},"trusted":true},"execution_count":null,"outputs":[]}]}